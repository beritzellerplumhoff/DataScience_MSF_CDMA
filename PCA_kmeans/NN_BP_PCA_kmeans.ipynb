{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389557e3",
   "metadata": {},
   "source": [
    "## Backpropagation in neural networks, PCA and k-means clustering\n",
    "\n",
    "Copyright (C) 2023-2025, B. Zeller-Plumhoff\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the [GNU General Public License](https://www.gnu.org/licenses/gpl-3.0.html) for more details.\n",
    "\n",
    "This Jupyter Notebook was created by Berit Zeller-Plumhoff. \n",
    "\n",
    "Within the notebook, you will display the architecture of a neural network. Following that, we will implement the PCA and k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550ddff",
   "metadata": {},
   "source": [
    "We begin by loading the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a437697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # library for organizing data\n",
    "import numpy as np # library for numerial computations\n",
    "from sklearn.model_selection import train_test_split# this library enables the splitting of a data set into training and test data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from numpy.linalg import eigh\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import matplotlib.pyplot as plt # library for plotting (not interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc820e82",
   "metadata": {},
   "source": [
    "### Predicting metallicity\n",
    "\n",
    "For classification of metallicit based on the material compositon, we will use the dataset based on the publication from [Zhuo et al., 2018](https://pubs.acs.org/doi/pdf/10.1021/acs.jpclett.8b00124)<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) that can be accessed through __matminer__. Have a look at last week's notebook to learn more about the dataset and __matminer__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368751b",
   "metadata": {},
   "source": [
    "In this case, we will start by loading the pickle file which we prepared previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1efda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '../data/ismetal_df.pkl'\n",
    "df=pd.read_pickle(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1eb46",
   "metadata": {},
   "source": [
    "Assign the dataframe __without__ columns \"formula\" \"is_metal\" and \"composition\" to your feature variable X and the \"is_metal\" column to the observation y. Then split this data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1592db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the variables\n",
    "X=df.drop([\"formula\",\"is_metal\",\"composition\"],axis=1)\n",
    "y=df[\"is_metal\"]\n",
    "\n",
    "# split your data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf69b1e",
   "metadata": {},
   "source": [
    "#### Neural network architecture\n",
    "\n",
    "As a next step we want to better understand the architecture of the best neural network that we have created. Lets assess the weights of our 4 most relevant features until now, which appeared to be the content of Se, S, O and Te, as well as a less important feature, such as H. Using the __get_loc__ functionality, find the column index they correspond to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols='H','Se','S','O','Te'\n",
    "for c in cols:\n",
    "    print(X.columns.get_loc(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12942d",
   "metadata": {},
   "source": [
    "How does the network architecture look, if you retrain the best classifier based on our initial train/test split? Can you draw a graph (including only these 5 input features)? To do so, use the NN_Vis.py file. The code in this file was taken from the entries of Milo and Oliver Wilken [here](https://stackoverflow.com/questions/29888233/how-to-visualize-a-neural-network/37366154#37366154), and is available under a [CC BY-SA 3.0 license](https://creativecommons.org/licenses/by-sa/3.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bfda619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN_Vis as VisNN\n",
    "clf=MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "network_structure = np.hstack(([5], np.asarray(clf.hidden_layer_sizes), [1]))\n",
    "\n",
    "# Draw the Neural Network with weights\n",
    "network=VisNN.DrawNN(network_structure)\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80132e6a",
   "metadata": {},
   "source": [
    "#### Partial training of network\n",
    "\n",
    "If you are interested in obtaining the test loss during training of your network, you may use the __partial_fit__ functionality of the MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa16a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=np.unique(y)\n",
    "# neural network (loop over number of hidden layers)\n",
    "model_nn=[]\n",
    "y_pred_nn=[]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "for k in range(1,3):\n",
    "    hl=np.ones((k))*10\n",
    "    model_nn_tmp = MLPClassifier(hidden_layer_sizes=(hl.astype(int)), max_iter=500, random_state=42)\n",
    "    lloss=[]\n",
    "    for j in range(1,501):\n",
    "        model_nn_tmp.partial_fit(X_train, y_train,classes=classes)\n",
    "        lloss.append(log_loss(y_test, model_nn_tmp.predict_proba(X_test)))\n",
    "    plt.plot(np.arange(1,501),lloss,label='# hidden layers =%s' %(int(k)))\n",
    "plt.ylabel('Test loss')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1, 1.02))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c24e15",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "\n",
    "In this part of the notebook, you will implement a PCA for the metallicity dataset. We begin by performing the standardization of our dataframe. For each feature $x$, compute $$x^{(i)}=\\frac{x^{(i)}-\\bar{x}}{\\sigma(x)}$$ where $\\bar{x}$ is the mean value of $x$ and $\\sigma(x)$ the standard deviation. Replace NaNs with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the variables\n",
    "X_standard=(X-np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "X_standard=X_standard.fillna(0)\n",
    "X_standard.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf6af2",
   "metadata": {},
   "source": [
    "The next step in the PCA is to compute the covariance matrix. Use the [__numpy covariance__](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) to do so. Display the covariance matrix, including axes labels and a colour bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_X_m=np.cov(X_standard,rowvar=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "im = ax.imshow(Cov_X_m)\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(np.shape(X_standard)[1]), labels=X_standard.columns)\n",
    "ax.set_yticks(np.arange(np.shape(X_standard)[1]), labels=X_standard.columns)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "ax.set_title(\"Covariance matrix based of composition features\")\n",
    "plt.colorbar(im,fraction=0.046, pad=0.04)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c98205",
   "metadata": {},
   "source": [
    "In the next step, we perform the singular value decomposition of the covariance matrix, i.e. we will determine $\\lambda$ and $v$ for which $$Cov_X \\cdot v = \\lambda v$$\n",
    "Use the function [__eigh__](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html) from numpy.linalg. Compute the **explained variance ratio (evr)** for the eigenvalues (singular values) and sort these in descending order.\n",
    "$$\n",
    "\\text{EVR}_k = \\frac{\\lambda_k}{\\sum_{i=1}^n \\lambda_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda_k$ is the eigenvalue corresponding to the $k$-th principal component.\n",
    "- $\\sum_{i=1}^n \\lambda_i$ is the total variance, i.e., the sum of all eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0ea453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine eigenvalues and eigenvectors\n",
    "egnvalues, egnvectors = eigh(Cov_X_m)\n",
    "evr=egnvalues/np.sum(egnvalues)\n",
    "sorted_idx = (evr).argsort()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720c216",
   "metadata": {},
   "source": [
    "Plot the explained variance ratio in the cumulative scree plot. Include a vertical line to highlight at which number of principal components 99% of the variance are explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817df30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "plt.plot(np.arange(len(evr)),np.cumsum(evr[sorted_idx]))\n",
    "plt.plot([np.min(np.where(np.cumsum(evr[sorted_idx])>0.99)),np.min(np.where(np.cumsum(evr[sorted_idx])>0.99))],[-0.01,1.01],color='red',linestyle='dashed')\n",
    "plt.title('Cumulative scree plot')\n",
    "plt.ylabel('Cumulative explained variance ratio')\n",
    "plt.xlabel('# principal components')\n",
    "plt.show()\n",
    "#print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527c875",
   "metadata": {},
   "source": [
    "Thus, instead of using the 103 features for predicting metallicity, we could transform them into their 78 most relevant principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a6f0c",
   "metadata": {},
   "source": [
    "### k-means clustering\n",
    "\n",
    "All the work we have performed until now in terms of classification was supervised machine learning, i.e. where we trained our classifiers with ground truth training data to predict unknown test data sets. We can also perform unsupervised machine learning. In this respect, we may use [__k-means__](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering, for example to cluster our metallicity dataset. Output the accuracy of the clusters you would predict. What are you observing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ad4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=1, n_init='auto').fit(X)\n",
    "print('Accuracy of k-means clustering:', np.round(accuracy_score(kmeans.labels_,y),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d2ea80",
   "metadata": {},
   "source": [
    "#### Defining K with Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e81f5",
   "metadata": {},
   "source": [
    "The **Within-Cluster Sum of Squares (WCSS)** is the sum of the squared distances between each data point and its corresponding cluster centroid, for all clusters. Mathematically, it is given by:\n",
    "\n",
    "$$\n",
    "\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x_j \\in C_i} (x_j - \\mu_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( k \\) is the number of clusters,\n",
    "- \\( C_i \\) is the \\(i\\)-th cluster,\n",
    "- \\( x_j \\) represents the data points in cluster \\( C_i \\),\n",
    "- \\( μ_i \\) is the centroid of the \\(i\\)-th cluster.\n",
    "\n",
    "The **Elbow Method** is used to plot the **WCSS** for different values of \\(k\\) and identify the optimal number of clusters. The \"elbow\" in the plot is the point at which the WCSS starts to decrease at a slower rate, indicating the optimal \\(k\\).\n",
    "\n",
    "\n",
    "In order to observe how k-means clustering and the elbow method works, we will use the iris dataset. Load the dataset and display sepal width vs. sepal length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a285dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "plt.scatter(data.data[:, 0], data.data[:, 1], c=data.target)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efda596",
   "metadata": {},
   "source": [
    "Perform the clustering for different $k$ and compute the distortion using __cdist__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d636b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K = range(1, 10)\n",
    " \n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k,n_init='auto').fit(X)\n",
    "    kmeanModel.fit(X)\n",
    " \n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
    "                                        'euclidean'), axis=1)) / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c901372",
   "metadata": {},
   "source": [
    "Plot the distortion for different values of k. Which k would you select?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method using Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02fa6a7",
   "metadata": {},
   "source": [
    "Let's plot how the __kmeans__ clustering is finding the clusters during fitting for 3 clusters for the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Ensure the 'kmeans' directory exists\n",
    "if not os.path.exists('kmeans'):\n",
    "    os.makedirs('kmeans')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for i in range(1, 51):\n",
    "    kmeans = KMeans(n_clusters=3, init='random', n_init=1, max_iter=i, random_state=1)\n",
    "    y_kmeans = kmeans.fit_predict(data.data)\n",
    "    ax.clear()\n",
    "    scatter = ax.scatter(data.data[:, 0], data.data[:, 1], c=y_kmeans)\n",
    "    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=100, c='red', label=\"Centroids\")  # Plot the centroids in red\n",
    "    \n",
    "    for j, centroid in enumerate(kmeans.cluster_centers_):\n",
    "        ax.text(centroid[0], centroid[1] + 0.1, f'C{j+1}', color='black', ha='center', fontsize=12)\n",
    "    \n",
    "    ax.set_title(f'K-means clustering (k=3) - Iteration {i}')\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.legend()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "plt.show() # if you want plot for each interaction just add tab here !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9663c8",
   "metadata": {},
   "source": [
    "<a name=\"cite_note-1\"></a>1.[^](#cite_ref-1) Y. Zhuo, A.M. Tehrani, and J. Brgoch, J. Phys. Chem. Lett. 2018, 9, 7, 1668–1673, https://doi.org/10.1021/acs.jpclett.8b00124"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
