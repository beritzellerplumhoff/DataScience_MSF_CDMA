{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677c2844",
   "metadata": {},
   "source": [
    "## Data Science Classification Exercise: MDS-2 (Light) Dataset\n",
    "Copyright (C) 2025, B. Zeller-Plumhoff\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the [GNU General Public License](https://www.gnu.org/licenses/gpl-3.0.html) for more details.\n",
    "\n",
    "This Jupyter Notebook was created by Berit Zeller-Plumhoff and Bianca Guedert. The code for importing the data set and displaying to examples was taken from the [MDS companion web page](https://mds-book.org/Content/datasets) - the corresponding software is published under the MIT license.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f9978",
   "metadata": {},
   "source": [
    "&#x1F50D; **Overview**: In this exercise, you will work with the **MDS-2 (Light)** dataset, which you can access from the [MDS Datasets](https://mds-book.org/Content/datasets) page. The goal is to apply classification algorithms to the dataset, following the steps and methods covered in class. \n",
    "\n",
    "&#x1F4D1; **Requirements**: Perform hyperparameter optimization (Grid Search) for **all the classifiers** we studied in the lectures\n",
    "\n",
    "&#x1F4C5; **Deadline**: You have until next week to complete this task. 28/01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58850b",
   "metadata": {},
   "source": [
    "### Objectives &#x2705;\n",
    "\n",
    "- [ ] Load the MDS-2 (Light) dataset and prepare it for classification.\n",
    "- [ ] Split the data into training, validation, and testing sets.\n",
    "- [ ] Apply **Grid Search** hyperparameter optimization for all the classifiers we covered in the lectures.\n",
    "- [ ] Evaluate the performance of each classifier and select the best model.\n",
    "- [ ] Compare the results using metrics like accuracy, confusion matrix.\n",
    "- [ ] Visualize the performance of the classifiers and their feature importances.\n",
    "- [ ] Compare your model results with [The Materials Data-Science Book](https://mds-book.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f04063",
   "metadata": {},
   "source": [
    "### Hands on &#129304;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9131146",
   "metadata": {},
   "source": [
    "&#128187; We begin by loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8d34f-c6cf-4440-9ad4-025d183ea453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # library for organizing data\n",
    "import numpy as np # library for numerial computations\n",
    "from sklearn import linear_model # the linear_model library establishes a straightforward implementation of a linear regression model\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score # this library enables the splitting of a data set into training and test data\n",
    "from sklearn.inspection import DecisionBoundaryDisplay # library to display decision boundaries of classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, RocCurveDisplay, roc_curve, roc_auc_score, log_loss, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt # library for plotting (not interactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e815ca5b",
   "metadata": {},
   "source": [
    "### Loading Ising Dataset &#x1F504;\n",
    "\n",
    "Load the Ising Light dataset, a smaller version of the original Ising dataset with 16x16px images, to explore the structure-property relationship. Access the [MDS-2 (Light) Dataset](https://mds-book.org/Content/datasets) to find more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690e16a-e373-4c8c-9821-311a37463a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdsdata import load_Ising_light  \n",
    "\n",
    "images, labels, temperatures = load_Ising_light()\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "ax0.imshow(images[10])\n",
    "ax0.set(title=f\"T={temperatures[10]:.2f}, label={labels[10]}\")\n",
    "ax1.imshow(images[3000])\n",
    "ax1.set(title=f\"T={temperatures[3000]:.2f}, label={labels[3000]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7548a7",
   "metadata": {},
   "source": [
    "### Data Processing and Splitting &#x1F4C8;\n",
    "\n",
    "The following code reshapes the image data and splits it into training, validation, and testing sets. This step is important to prepare the data for further classification tasks, including performing grid search for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23614b2-0615-49b6-933f-80a38e3e9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=images.reshape(5000,-1,1)[:,:,0] # you can select a fraction of the images initially in order to reduce the amount \n",
    "# of computational time while testing your code\n",
    "y=labels\n",
    "\n",
    "# split your data set into training/validation and testing - from here you can perform further splits for your grid hyperparameter search\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e3c50",
   "metadata": {},
   "source": [
    "### Grid Search Hyperparameter Tuning\n",
    "\n",
    "Perform Grid Search hyperparameter optimization for all the classifiers discussed in class.\n",
    "\n",
    "&#x1F4A1; **Tip**: In `sklearn`, the **Pipeline** feature allows you to chain multiple steps (e.g., data preprocessing and model training) into a single object. This ensures that your transformations (like scaling or encoding) are consistently applied during both training and testing. It makes your workflow more efficient, reduces errors, and keeps your code clean! \n",
    "\n",
    "To learn more about `Pipeline`, check out the official [sklearn documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) ðŸ“–."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad136e",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "After finding the best hyperparameters for each classifier, evaluate the model performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968da084",
   "metadata": {},
   "source": [
    "### If you find necessary keep adding titles to organize you code !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
